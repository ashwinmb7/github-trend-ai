{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c805da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train/Test Split Complete!\n",
      "Training set: 1269 samples (80.0%)\n",
      "Testing set:  318 samples (20.0%)\n",
      "\n",
      "Shapes:\n",
      "  X_train: (1269, 16), y_train: (1269,)\n",
      "  X_test:  (318, 16), y_test:  (318,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load processed dataset\n",
    "df = pd.read_csv(\"../data/processed/processed_dataset.csv\")\n",
    "\n",
    "# Create target and features\n",
    "target = 'stars_period'\n",
    "y = df[target]\n",
    "X = df.drop(columns=[target])\n",
    "\n",
    "# Split into train and test sets (80/20)\n",
    "#80% used for training, 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Train/Test Split Complete!\")\n",
    "print(f\"Training set: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Testing set:  {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "print(f\"\\nShapes:\")\n",
    "print(f\"  X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"  X_test:  {X_test.shape}, y_test:  {y_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a5b4d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Linear Regression ---\n",
      "Mean Squared Error: 1283742.76\n",
      "R^2 Score: 0.01\n",
      "\n",
      "--- Random Forest Regressor ---\n",
      "Mean Squared Error: 1672773.27\n",
      "R^2 Score: -0.29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  simple baseline model using linear regression and random forest classifier\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Linear Regression Model \n",
    "lr_model = LinearRegression()\n",
    "\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "\n",
    "# Random Forest Regressor Model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate Models\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"--- {model_name} ---\")\n",
    "    print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "    print(f\"R^2 Score: {r2:.2f}\\n\")\n",
    "\n",
    "evaluate_model(y_test, y_pred_lr, \"Linear Regression\")\n",
    "evaluate_model(y_test, y_pred_rf, \"Random Forest Regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c826729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing xgboost...\n",
      "============================================================\n",
      "XGBOOST MODEL - Training & Evaluation\n",
      "============================================================\n",
      "\n",
      "üîÑ Training XGBoost model...\n",
      "\n",
      "üìä XGBoost Results:\n",
      "  Training Set:\n",
      "    MSE:  54455.96\n",
      "    RMSE: 233.36\n",
      "    MAE:  102.45\n",
      "    R¬≤:   0.9745\n",
      "\n",
      "  Test Set:\n",
      "    MSE:  1400337.62\n",
      "    RMSE: 1183.36\n",
      "    MAE:  358.98\n",
      "    R¬≤:   -0.0812\n",
      "\n",
      "============================================================\n",
      "‚úÖ XGBoost model trained!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# XGBoost Model - Better performance for regression\n",
    "# Install if needed: !pip install xgboost\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except ImportError:\n",
    "    print(\"Installing xgboost...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([\"pip\", \"install\", \"xgboost\"])\n",
    "    import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"XGBOOST MODEL - Training & Evaluation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# XGBoost with tuned hyperparameters\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=200,           # Number of trees\n",
    "    max_depth=6,                # Maximum tree depth\n",
    "    learning_rate=0.1,         # Learning rate\n",
    "    subsample=0.8,              # Row sampling\n",
    "    colsample_bytree=0.8,       # Column sampling\n",
    "    min_child_weight=3,         # Minimum samples in leaf\n",
    "    random_state=42,\n",
    "    n_jobs=-1                   # Use all CPU cores\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nüîÑ Training XGBoost model...\")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "xgb_train_pred = xgb_model.predict(X_train)\n",
    "xgb_test_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "xgb_train_mse = mean_squared_error(y_train, xgb_train_pred)\n",
    "xgb_test_mse = mean_squared_error(y_test, xgb_test_pred)\n",
    "xgb_train_rmse = np.sqrt(xgb_train_mse)\n",
    "xgb_test_rmse = np.sqrt(xgb_test_mse)\n",
    "xgb_train_mae = mean_absolute_error(y_train, xgb_train_pred)\n",
    "xgb_test_mae = mean_absolute_error(y_test, xgb_test_pred)\n",
    "xgb_train_r2 = r2_score(y_train, xgb_train_pred)\n",
    "xgb_test_r2 = r2_score(y_test, xgb_test_pred)\n",
    "\n",
    "print(\"\\nüìä XGBoost Results:\")\n",
    "print(f\"  Training Set:\")\n",
    "print(f\"    MSE:  {xgb_train_mse:.2f}\")\n",
    "print(f\"    RMSE: {xgb_train_rmse:.2f}\")\n",
    "print(f\"    MAE:  {xgb_train_mae:.2f}\")\n",
    "print(f\"    R¬≤:   {xgb_train_r2:.4f}\")\n",
    "print(f\"\\n  Test Set:\")\n",
    "print(f\"    MSE:  {xgb_test_mse:.2f}\")\n",
    "print(f\"    RMSE: {xgb_test_rmse:.2f}\")\n",
    "print(f\"    MAE:  {xgb_test_mae:.2f}\")\n",
    "print(f\"    R¬≤:   {xgb_test_r2:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ XGBoost model trained!\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa37facc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MODEL COMPARISON - All Models\n",
      "======================================================================\n",
      "\n",
      "Model                MSE             RMSE            MAE             R¬≤        \n",
      "----------------------------------------------------------------------\n",
      "Linear Regression    1283742.76      1133.02         396.80          0.0088    \n",
      "Random Forest        1672773.27      1293.36         308.79          -0.2916   \n",
      "XGBoost              1400337.62      1183.36         358.98          -0.0812   \n",
      "\n",
      "üèÜ Best Model: Linear Regression (Lowest MSE: 1283742.76)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Compare All Models Side-by-Side\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL COMPARISON - All Models\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get predictions from all models (if they exist)\n",
    "models_comparison = []\n",
    "\n",
    "# Linear Regression\n",
    "if 'y_pred_lr' in globals():\n",
    "    lr_mse = mean_squared_error(y_test, y_pred_lr)\n",
    "    lr_rmse = np.sqrt(lr_mse)\n",
    "    lr_r2 = r2_score(y_test, y_pred_lr)\n",
    "    lr_mae = mean_absolute_error(y_test, y_pred_lr)\n",
    "    models_comparison.append((\"Linear Regression\", lr_mse, lr_rmse, lr_mae, lr_r2))\n",
    "\n",
    "# Random Forest\n",
    "if 'y_pred_rf' in globals():\n",
    "    rf_mse = mean_squared_error(y_test, y_pred_rf)\n",
    "    rf_rmse = np.sqrt(rf_mse)\n",
    "    rf_r2 = r2_score(y_test, y_pred_rf)\n",
    "    rf_mae = mean_absolute_error(y_test, y_pred_rf)\n",
    "    models_comparison.append((\"Random Forest\", rf_mse, rf_rmse, rf_mae, rf_r2))\n",
    "\n",
    "# XGBoost\n",
    "if 'xgb_test_pred' in globals():\n",
    "    models_comparison.append((\"XGBoost\", xgb_test_mse, xgb_test_rmse, xgb_test_mae, xgb_test_r2))\n",
    "\n",
    "# Print comparison table\n",
    "print(f\"\\n{'Model':<20} {'MSE':<15} {'RMSE':<15} {'MAE':<15} {'R¬≤':<10}\")\n",
    "print(\"-\" * 70)\n",
    "for name, mse, rmse, mae, r2 in models_comparison:\n",
    "    print(f\"{name:<20} {mse:<15.2f} {rmse:<15.2f} {mae:<15.2f} {r2:<10.4f}\")\n",
    "\n",
    "# Find best model\n",
    "if models_comparison:\n",
    "    best_model = min(models_comparison, key=lambda x: x[1])  # Lowest MSE\n",
    "    print(f\"\\nüèÜ Best Model: {best_model[0]} (Lowest MSE: {best_model[1]:.2f})\")\n",
    "    \n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77637f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "XGBOOST FEATURE IMPORTANCE\n",
      "============================================================\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "------------------------------------------------------------\n",
      "log_forks                      0.2443\n",
      "forks                          0.2257\n",
      "timeframe_encoded              0.1110\n",
      "normalized_forks               0.0882\n",
      "stars                          0.0733\n",
      "log_stars                      0.0681\n",
      "search_language_encoded        0.0528\n",
      "popularity_score               0.0518\n",
      "language_encoded               0.0480\n",
      "stars_forks_ratio              0.0260\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# XGBoost Feature Importance - See which features matter most\n",
    "if 'xgb_model' in globals():\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': xgb_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"XGBOOST FEATURE IMPORTANCE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(\"-\" * 60)\n",
    "    for idx, row in feature_importance.head(10).iterrows():\n",
    "        print(f\"{row['feature']:<30} {row['importance']:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
